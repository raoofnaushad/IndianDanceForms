{"cells":[{"metadata":{},"cell_type":"markdown","source":"### Libraries","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom PIL import Image\nfrom tqdm import tqdm\nimport random\n\nfrom sklearn import model_selection\n\nimport torch\nimport torchvision.models as models\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom torchvision.transforms import transforms\nfrom torchvision.utils import make_grid\nimport torch.nn.functional as F","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Config Files","execution_count":null},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"base_path = '../input/version-8-indian-dance-forms/IndianDanceForms2'\ntrain_dir = os.path.join(base_path, 'train')\ntest_dir = os.path.join(base_path, 'test')\ntrain_df = pd.read_csv(os.path.join(base_path, 'train.csv'))\ntest_df = pd.read_csv(os.path.join(base_path, 'test.csv'))\nvalid_df = pd.read_csv(os.path.join(base_path, 'valid.csv'))\nvalid_dir = os.path.join(base_path, 'test')\ntest_df['target'] = ['manipuri']*len(test_df)\nif not os.path.isdir('output'):\n    os.mkdir('output')\nouput_dir = 'output'\n\nidx_class_labels = {\n    0: 'bharatanatyam',\n    1: 'kathak', \n    2: 'kathakali',\n    3: 'kuchipudi',\n    4: 'manipuri',\n    5: 'mohiniyattam',\n    6: 'odissi',\n    7: 'sattriya'\n}\nclass_idx_labels = {\n    'bharatanatyam': 0,\n    'kathak': 1,\n    'kathak ': 1,\n    'kathakali': 2,\n    'kuchipudi': 3,\n    'manipuri': 4,\n    'mohiniyattam': 5,\n    'odissi': 6,\n    'sattriya': 7\n}\n\nnum_classes = len(idx_class_labels.items())\nval_size = 0.1\n## Model Config\ntorch.manual_seed(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Setting seeds for deterministic results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"SEED = 1234\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Utility Functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def show_sample(img, target=None):\n    if target:\n        print(\"Label\" ,decode_target(int(target), text_labels=True))\n    plt.imshow(img.permute(1, 2, 0))\n\ndef show_difference(img1, target1, img2, target2):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n    ax1.set_title(\"Before Transformation\")\n    ax1.imshow(img1.permute(1, 2 ,0))\n    ax2.set_title(\"After Transformation\")\n    ax2.imshow(img2.permute(1, 2 ,0))\n    \ndef show_batch(dl):\n    for images, labels in dl:\n        fig, ax = plt.subplots(figsize=(16, 8))\n        ax.set_xticks([]); ax.set_yticks([])\n        ax.imshow(make_grid(images, nrow=16).permute(1, 2, 0))\n        break\n        \ndef encode_label(label):\n    idx = class_idx_labels[label] \n    return idx\n\ndef decode_target(target, text_labels=True):\n    result = []\n    if text_labels:\n        return idx_class_labels[target]\n    else:\n        return target\n## Example\nprint(encode_label('kathakali'))\nprint( decode_target(2, text_labels=True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating Dataset and Dataloaders\n### Now we will Create custom dataset by extending pytorch Dataset class. We also add provision for adding transformers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class IndianDanceForms(Dataset):\n    def __init__(self, train_df, train_dir, transform=None):\n        self.train_dir = train_dir\n        self.train_df = train_df\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.train_df)\n    \n    def __getitem__(self, idx):\n        row = self.train_df.loc[idx]\n        img_id, label = row['Image'], row['target']\n        img = Image.open(os.path.join(self.train_dir, img_id))\n        if self.transform:\n            img = self.transform(img)\n        return img, encode_label(label)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Applying Transformations","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"padding = (10, 15, 20, 30)\ntrain_transform = transforms.Compose([\n                                transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n                                transforms.RandomHorizontalFlip(),\n                                transforms.RandomRotation(degrees=(-10,+10)),\n                                transforms.RandomAffine(0, translate=None, scale=(0.9, 1.1), shear=1, resample=False, fillcolor=0),\n                                transforms.Pad(padding, fill=0, padding_mode='reflect'),\n                                transforms.Resize(size=(224, 224)),\n                                transforms.ToTensor(),\n                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n                                ])\nvalid_transform = transforms.Compose([\n                                transforms.Resize(size=(224, 224)),\n                                transforms.ToTensor(),\n                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n                                ])\ntrain_ds = IndianDanceForms(train_df, train_dir, train_transform)\nvalid_ds = IndianDanceForms(valid_df, valid_dir, valid_transform)\nprint(len(train_ds), len(valid_ds))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Creating Dataloaders","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 64\ntrain_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=2, pin_memory=True)\nvalid_dl = DataLoader(valid_ds, batch_size, shuffle=False, num_workers=2, pin_memory=True)\nshow_batch(train_dl)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Building Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim = 1)\n    return torch.tensor(torch.sum(preds==labels).item() / len(preds))\n\nclass MulticlassClassifierBase(nn.Module):\n    \n    def training_step(self, batch):\n        img, label = batch\n        out = self(img)\n        loss = criterion(out, label)\n        accu = accuracy(out, label)\n        return accu ,loss\n    def validation_step(self, batch):\n        img, label = batch\n        out = self(img)\n        loss = criterion(out, label)\n        accu = accuracy(out, label)\n        return {\"val_loss\": loss.detach(), \"val_acc\": accu}\n    \n    def validation_epoch_ends(self, outputs):\n        batch_loss = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_loss).mean()\n        batch_acc = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_acc).mean()\n        return {\"val_loss\":epoch_loss.item(), \"val_acc\":epoch_acc.item()}\n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}],train_accu: {:.4f}, learning_rate: {:.4f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n            epoch,result['train_accu'], result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class DanceFormModel(MulticlassClassifierBase):\n    def __init__(self):\n        super().__init__()\n        self.network = models.wide_resnet50_2(pretrained=True)\n        n_inputs = self.network.fc.in_features\n        self.network.fc = nn.Sequential(\n                              nn.Linear(n_inputs, 256),\n                              nn.ReLU(),\n                              nn.Dropout(0.5),\n                              nn.Linear(256, 8),\n                              nn.LogSoftmax(dim=1)\n                                )\n    def forward(self, xb):\n        return self.network(xb)\n    \n    def freeze(self):\n        for param in self.network.parameters():\n            param.require_grad=False\n        for param in self.network.fc.parameters():\n            param.require_grad=True\n    def unfreeze(self):\n        for param in self.network.parameters():\n            param.require_grad=True\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = DanceFormModel()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Checking how each batch goes through model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def try_batch(dl):\n    for images, labels in dl:  \n        print(images.shape)\n        out = model(images)\n        print(out.shape)\n        print(out[0])\n        break\ntry_batch(train_dl)    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training and Evaluating","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"@torch.no_grad()\ndef evaluate(model, valid_loader):\n    model.eval()\n    outputs = [model.validation_step(batch) for batch in valid_loader]\n    return model.validation_epoch_ends(outputs)\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group['lr']\n    \ndef fit(epochs, max_lr,  model, train_loader, valid_loader, weight_decay=0, grad_clip=None,opt_func=torch.optim.SGD, max_epochs_stop=3):\n    history = []\n    \n    valid_loss_min = np.Inf\n    valid_acc_max = 0\n    model_file_name = 'indian_dance_form.pt'\n    model_file_name2 = 'indian_dance_form_max_acc.pt'\n    epochs_no_improve =  0\n    optimizer = opt_func(model.parameters(), lr=max_lr, weight_decay=weight_decay)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.00001)\n                         \n    for epoch in range(epochs):\n        model.train()\n        train_loss = []\n        train_accu = []\n        lrs = []\n        for batch in tqdm(train_loader):\n            \n            accu, loss = model.training_step(batch)\n            train_loss.append(loss)\n            train_accu.append(accu)\n            loss.backward()\n            ## Gradient Clipping\n            if grad_clip:\n                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n            \n            optimizer.step()\n            optimizer.zero_grad()\n            \n            lrs.append(get_lr(optimizer))\n            \n            \n            \n        result = evaluate(model, valid_loader)\n        ########### Early Stopping ##############                                         \n        valid_loss = result['val_loss']\n        valid_acc = result['val_acc']\n        if valid_acc > valid_acc_max:\n            torch.save(model.state_dict(), model_file_name2)\n            valid_acc_max = valid_acc\n        if valid_loss<valid_loss_min:\n            torch.save(model.state_dict(), model_file_name)\n            valid_loss_min = valid_loss                                  \n            epochs_no_improve = 0          \n        else:\n            epochs_no_improve += 1\n            if epochs_no_improve > max_epochs_stop:\n                result[\"train_loss\"] = torch.stack(train_loss).mean().item()\n                result[\"train_accu\"] = torch.stack(train_accu).mean().item()\n                result[\"lrs\"] = lrs\n                model.epoch_end(epoch, result)\n                history.append(result)\n                print(\"Early Stopping............................\")\n                return history                                \n                                                 \n        result[\"train_loss\"] = torch.stack(train_loss).mean().item()\n        result[\"train_accu\"] = torch.stack(train_accu).mean().item()\n        result[\"lrs\"] = lrs\n        model.epoch_end(epoch, result)\n        history.append(result)\n    print(\"VAL LOSS MIN {}\".format(valid_loss_min))\n    print(\"VAL ACC MAX {}\".format(valid_acc_max))\n    return history","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Initializing Device also Loading Data and Model to device","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_device():\n    print(torch.cuda.is_available())\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\ndef to_device(data, device):\n    if isinstance(data, (list, tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n        \n    def __iter__(self):\n        for b in self.dl:\n            yield to_device(b, self.device)\n            \n    def __len__(self):\n        return len(self.dl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = get_device()\n## Loading data to devide\ntrain_dl = DeviceDataLoader(train_dl, device)\nvalid_dl = DeviceDataLoader(valid_dl, device)\n## Loading model to device\nmodel = to_device(DanceFormModel(), device)\n## lets try passing a batch to model again\ntry_batch(train_dl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"## Hyper Parameters\nmax_epochs_stop = 10\nmax_lr = 1e-4\ngrad_clip = 0.1\nweight_decay = 1e-3\nbatch_size = 64\ncriterion = nn.CrossEntropyLoss()\nepochs = 2\nopt_func = torch.optim.Adam\n## Evaluating with non-trained model\nevaluate(model, valid_dl)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Freezing except last layer\nmodel.freeze()\n## Training\nhistory = fit(epochs, max_lr, model, train_dl, valid_dl, weight_decay, grad_clip, opt_func, max_epochs_stop)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualize","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_losses(history):\n    train_losses = [x.get('train_loss') for x in history]\n    val_losses = [x['val_loss'] for x in history]\n    plt.plot(train_losses, '-bx')\n    plt.plot(val_losses, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Loss vs. No. of epochs');\n    \nplot_losses(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_accuracy(history):\n        \n    train_accu = [x.get('train_accu') for x in history]\n    val_accu = [x['val_acc'] for x in history]\n    plt.plot(train_accu, '-bx')\n    plt.plot(val_accu, '-rx')\n    plt.xlabel('epoch')\n    plt.ylabel('Accuracy')\n    plt.legend(['Training', 'Validation'])\n    plt.title('Accuracy vs. No. of epochs');\nplot_accuracy(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_lrs(history):\n    lrs = np.concatenate([x.get('lrs', []) for x in history])\n    plt.plot(lrs)\n    plt.xlabel('Batch no.')\n    plt.ylabel('Learning rate')\n    plt.title('Learning Rate vs. Batch no.');\n    \nplot_lrs(history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Predict Single Images\ndef predict_single(image):\n    xb = image.unsqueeze(0)\n    xb = to_device(xb, device)\n    preds = model(xb)\n    _, prediction = torch.max(preds.cpu().detach(), dim=1)\n    print(\"Prediction: \", int(prediction), decode_target(int(prediction), text_labels=True))\n    show_sample(image, prediction)\n    return decode_target(int(prediction), text_labels=True)\n    \npredict_single(valid_ds[1][0])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}